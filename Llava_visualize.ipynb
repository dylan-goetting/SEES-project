{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0b19815-45c9-47da-868a-cedcd7c4df73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import cv2\n",
    "import torch\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "LAYER_NUM = 32\n",
    "HEAD_NUM = 32\n",
    "HEAD_DIM = 128\n",
    "HIDDEN_DIM = HEAD_NUM * HEAD_DIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58a0452c-ebd5-40fc-8b86-efc139cad707",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from transformers.image_transforms import (\n",
    "    convert_to_rgb,\n",
    "    get_resize_output_image_size,\n",
    "    resize,\n",
    "    center_crop)\n",
    "from transformers.image_utils import (\n",
    "    infer_channel_dimension_format,\n",
    "    to_numpy_array)\n",
    "def normalize(vector):\n",
    "    max_value = max(vector)\n",
    "    min_value = min(vector)\n",
    "    vector1 = [(x-min_value)/(max_value-min_value) for x in vector]\n",
    "    vector2 = [x/sum(vector1) for x in vector1]\n",
    "    return vector2\n",
    "def get_bsvalues(vector, model, final_var):\n",
    "    vector = vector * torch.rsqrt(final_var + 1e-6)\n",
    "    vector_rmsn = vector * model.language_model.model.norm.weight.data\n",
    "    vector_bsvalues = model.language_model.lm_head(vector_rmsn).data\n",
    "    return vector_bsvalues\n",
    "def get_prob(vector):\n",
    "    prob = torch.nn.Softmax(-1)(vector)\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50cbdeac-d3d4-46ab-9c64-23901c5ed380",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a96de8fec68b4762a54d32554cfc5688",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "LlavaForConditionalGeneration(\n",
       "  (vision_tower): CLIPVisionModel(\n",
       "    (vision_model): CLIPVisionTransformer(\n",
       "      (embeddings): CLIPVisionEmbeddings(\n",
       "        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "        (position_embedding): Embedding(577, 1024)\n",
       "      )\n",
       "      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (encoder): CLIPEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-23): 24 x CLIPEncoderLayer(\n",
       "            (self_attn): CLIPAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): CLIPMLP(\n",
       "              (activation_fn): QuickGELUActivation()\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (multi_modal_projector): LlavaMultiModalProjector(\n",
       "    (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "    (act): GELUActivation()\n",
       "    (linear_2): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "  )\n",
       "  (language_model): LlamaForCausalLM(\n",
       "    (model): LlamaModel(\n",
       "      (embed_tokens): Embedding(32064, 4096)\n",
       "      (layers): ModuleList(\n",
       "        (0-31): 32 x LlamaDecoderLayer(\n",
       "          (self_attn): LlamaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (rotary_emb): LlamaRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "            (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm()\n",
       "          (post_attention_layernorm): LlamaRMSNorm()\n",
       "        )\n",
       "      )\n",
       "      (norm): LlamaRMSNorm()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=4096, out_features=32064, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_id = \"../scratch/save_models/llava-7b\"\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "model = LlavaForConditionalGeneration.from_pretrained(model_id)\n",
    "model.eval()\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbff1f04-919e-4954-874e-e4338aa3495f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vqa_predict1(image, prompt):\n",
    "    inputs = processor(text=prompt, images=image, return_tensors=\"pt\")\n",
    "    inputs.to(\"cuda\")\n",
    "    outputs = model(**inputs)\n",
    "    outputs_probs = get_prob(outputs[\"logits\"][0][-1])\n",
    "    outputs_probs_sort = torch.argsort(outputs_probs, descending=True)\n",
    "    final_var = outputs[2][LAYER_NUM-1][4][0][-1].pow(2).mean(-1, keepdim=True)\n",
    "\n",
    "    resample = 3\n",
    "    shortest_edge = 336\n",
    "    crop_size = {\"height\": 336, \"width\": 336}\n",
    "    image_convert = convert_to_rgb(image)\n",
    "    image_numpy = to_numpy_array(image_convert)\n",
    "    input_data_format = infer_channel_dimension_format(image_numpy)\n",
    "    output_size = get_resize_output_image_size(image_numpy, size=336,\n",
    "                default_to_square=False, input_data_format=input_data_format)\n",
    "    image_resize = resize(image_numpy, output_size, resample=resample, input_data_format=input_data_format)\n",
    "    image_center_crop = center_crop(image_resize, size=(crop_size[\"height\"], crop_size[\"width\"]), input_data_format=input_data_format)\n",
    "    img3 = Image.fromarray(image_center_crop)\n",
    "    img3.save(\"tmp.png\")\n",
    "\n",
    "    predict_index = outputs_probs_sort[0]\n",
    "    all_head_increase = []\n",
    "    for test_layer in range(LAYER_NUM):\n",
    "        cur_layer_input = outputs[2][test_layer][0][0]\n",
    "        cur_v_heads = outputs[2][test_layer][5][0]\n",
    "        cur_attn_o_split = model.language_model.model.layers[test_layer].self_attn.o_proj.weight.data.T.view(HEAD_NUM, HEAD_DIM, -1)\n",
    "        cur_attn_subvalues_headrecompute = torch.bmm(cur_v_heads, cur_attn_o_split).permute(1, 0, 2)\n",
    "        cur_attn_subvalues_head_sum = torch.sum(cur_attn_subvalues_headrecompute, 0)\n",
    "        cur_layer_input_last = cur_layer_input[-1]\n",
    "        origin_prob = torch.log(get_prob(get_bsvalues(cur_layer_input_last, model, final_var))[predict_index])\n",
    "        cur_attn_subvalues_head_plus = cur_attn_subvalues_head_sum + cur_layer_input_last\n",
    "        cur_attn_plus_probs = torch.log(get_prob(get_bsvalues(\n",
    "                cur_attn_subvalues_head_plus, model, final_var))[:, predict_index])\n",
    "        cur_attn_plus_probs_increase = cur_attn_plus_probs - origin_prob\n",
    "        for i in range(len(cur_attn_plus_probs_increase)):\n",
    "            all_head_increase.append([str(test_layer)+\"_\"+str(i), round(cur_attn_plus_probs_increase[i].item(), 4)])\n",
    "    all_head_increase_sort = sorted(all_head_increase, key=lambda x:x[-1])[::-1]\n",
    "\n",
    "    test_layer, head_index = all_head_increase_sort[0][0].split(\"_\")\n",
    "    test_layer, head_index = int(test_layer), int(head_index)\n",
    "    cur_layer_input = outputs[2][test_layer][0][0]\n",
    "    cur_v_heads = outputs[2][test_layer][5][0]\n",
    "    cur_attn_o_split = model.language_model.model.layers[test_layer].self_attn.o_proj.weight.data.T.view(HEAD_NUM, HEAD_DIM, -1)\n",
    "    cur_attn_subvalues_headrecompute = torch.bmm(cur_v_heads, cur_attn_o_split).permute(1, 0, 2)\n",
    "    cur_attn_subvalues_headrecompute_curhead = cur_attn_subvalues_headrecompute[:, head_index, :]\n",
    "    cur_layer_input_last = cur_layer_input[-1]\n",
    "    origin_prob = torch.log(get_prob(get_bsvalues(\n",
    "        cur_layer_input_last, model, final_var))[predict_index])\n",
    "    cur_attn_subvalues_headrecompute_curhead_plus = cur_attn_subvalues_headrecompute_curhead + cur_layer_input_last\n",
    "    cur_attn_plus_probs = torch.log(get_prob(get_bsvalues(\n",
    "        cur_attn_subvalues_headrecompute_curhead_plus, model, final_var))[:, predict_index])\n",
    "    cur_attn_plus_probs_increase = cur_attn_plus_probs - origin_prob\n",
    "    head_pos_increase = cur_attn_plus_probs_increase.tolist()\n",
    "    curhead_increase_scores = head_pos_increase[5:581]\n",
    "    increase_scores_normalize = normalize(curhead_increase_scores)\n",
    "\n",
    "    attn_scores_all = torch.tensor([0.0]*576).to(\"cuda\")\n",
    "    for layer_index in range(LAYER_NUM):\n",
    "        for head_index in range(HEAD_NUM):\n",
    "            attn_scores = outputs[2][layer_index][7][0][head_index][-1][5:581]\n",
    "            attn_scores_all += attn_scores\n",
    "    attn_scores_all = attn_scores_all/1024.0\n",
    "\n",
    "    demo_img = plt.imread(\"tmp.png\")\n",
    "    demo_img_h, demo_img_w, demo_img_c = demo_img.shape\n",
    "    demo_img_inc = np.array(increase_scores_normalize).reshape((24, 24))\n",
    "    demo_img_inc = cv2.resize(demo_img_inc,\n",
    "                              dsize=(demo_img_w, demo_img_h),\n",
    "                              interpolation=cv2.INTER_CUBIC)\n",
    "    demo_img_att_avg = np.array(attn_scores_all.tolist()).reshape((24, 24))\n",
    "    demo_img_att_avg = cv2.resize(demo_img_att_avg,\n",
    "                              dsize=(demo_img_w, demo_img_h),\n",
    "                              interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "    plt.figure(figsize=(60, 30))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(demo_img)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"input image\", fontsize=40)\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(demo_img)\n",
    "    plt.imshow(demo_img_inc, alpha=0.8, cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"log probability increase\", fontsize=40)\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(demo_img)\n",
    "    plt.imshow(demo_img_att_avg, alpha=0.8, cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"avg attention score\", fontsize=40)\n",
    "    plt.savefig(\"tmp1.png\")\n",
    "    plt.close()\n",
    "    image_show = Image.open(\"tmp1.png\")\n",
    "\n",
    "    prediction = processor.decode(outputs_probs_sort[0])\n",
    "    top_heads = \"important heads: \"+str([(x[0], round(x[1],4)) for x in all_head_increase_sort[:10]])\n",
    "    important_image_batches = image_show\n",
    "\n",
    "    return prediction, top_heads, important_image_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b870a5f6-09c1-46b8-a3c8-2342dcee434e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/iusers01/nactem01/y35866zy/anaconda3/envs/hugging/lib/python3.8/site-packages/gradio/analytics.py:106: UserWarning: IMPORTANT: You are using gradio version 4.42.0, however version 4.44.1 is available, please upgrade. \n",
      "--------\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on public URL: https://66ac6212a844030a18.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://66ac6212a844030a18.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#download the test example and upload: http://images.cocodataset.org/val2017/000000219578.jpg\n",
    "#please use prompts similar to this to fit Llava: \"USER: <image>\\nWhat is the color of the dog?\\nASSISTANT: The color of the dog is\"\n",
    "\n",
    "gr.Interface(fn=vqa_predict1, inputs=[\"image\", \"text\"], outputs=[\"text\", \"text\", \"image\"]).launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36200f6c-439b-455a-80d6-76dd9508cf3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4c55b9-4b78-4f06-8a9b-8158fc311f2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187f1716-615a-4307-b6cc-022451cbcf13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a0a570-b7ba-4d4d-b539-3fe973299637",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf8852d-8387-4055-a2d5-a7c35175d49f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027938ba-b1aa-42dc-a111-465dbf2d14a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef62eae-9732-4035-8ab4-7d7b912b5fcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337118e9-888b-41f9-a384-2e8e893f179e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db6e011-d1d0-4bd9-92b5-c3237aaeb328",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6c878f-1ce6-4665-8ee7-817c5fa38d0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
